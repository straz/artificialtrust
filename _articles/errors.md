---
layout: article
title: Fatal Darwinism
released: true
status: ready
defs: [semantic]
arts: [cyber, kryptonite]
sent: ['lessig','colbert']
---

Neural nets work on a fairly straightforward principal of applied
Darwinism.  They are trained by rewarding correct responses, and
punishing incorrect ones.

Darwinism relies on two conditions: (1) the decision-maker faces the
fatal effects of poor decisions, and (2) the individual's suffering is
inconsequential as long as the species is sustained. This is all fine
and good to create life on Earth as we know it. It is not fine and
good when we deploy AIs in situations that have high-stakes impact on
people's lives, because neither of these conditions are true.

In nature, errors in a neural net are tolerated as long as they don't
happen too frequently.  For organisms, each individual is its own neural net, 
perhaps one of billions of copies of a species.

If one of them makes a fatal mistake, while it's certainly bad news
for that individual, the species it belongs to is typically not
threatened. In fact, if the environment is fairly stable and the
errors don't happen too frequently, the species will likely benefit
from the selective pressure, which in turn improves that neural net,
slowly, over time.

When we deploy artificial neural nets, they too will inevitably make
errors.  If the stakes are low, the systems we build are not
threatened by the occasional error. And neither are we, the humans who
interact with such systems.  For example, if an AI decides to show a
pet food commercial to someone who doesn't own any pets, that's really
not a big deal.

In terms of the two conditions I mentioned earlier, this sort of
low-stakes neural net AI situation affects two parties: the commercial
sponsor and the shopper.  We may allow a generous interpretation of
the first condition: an erroneous AI classification causes a (minor)
loss in sales to the sponsor, and also the shopper's time and
attention is wasted (just a bit). On one hand, the sponsor in this
case does indeed face proportional consequences from the error, but to
be accurate, we must also note that the shopper pays a proportionate
price as well, and here's the import part, without any accountability
or consequences to the AI *or its sponsor* for that portion of the cost.

This brings us to the second condition: the position that the
individual's suffering is inconsequential. We might very well accept
this position for a banner ad placement, but that's a slippery slope
upon which to build and widely establish a technology.

The stakes are high when decisions are made that affect healthcare,
justice, and money, or in other words, life, liberty, and the pursuit
of happiness.

A single mistake here causes no harm to the AI, which has no stake
in the outcome. But it is absolutely devastating to the human whose
life it affects. 

#### Policy implication #1: 

Before deploying a neural net AI with potentially high-stakes
consequences, model the prevalence and impact of errors it will make,
including both false positives and false negatives.  After it is
deployed, independently audit and publish the actual prevalence and
impact.

#### Policy implication #2: 

The impact of an error made by a neural net AI is felt not just by the
AI's sponsor (typically a corporation or government entity), but also
by the individuals who are the subjects of the AI's classification
decisions.  The distribution of harm between the sponsors and the
subjects should be accurately described.

#### Policy implication #3: 

While the aggregate impact of errors on subjects at population scale
might be very low, the individuals confronting these errors may bear a
very heavy burden from individual errors. Before deploying a neural net AI
with potentially high-stakes consequences, the impact of a single error
on a single subject should be accurately described as part of the 
description of the AI's impact.
