---
layout: article
title: Remove bias? Not so fast...
released: false
defs: ['reason-do-die', 'action', 'brakes']
---

First, there are two kinds of intelligence, which I call “do or die” and "reason why"

  “Do or die” responses are knee-jerk survival reflexes, epitomized by neural nets.

  Knee-jerk survival reflexes are based on the labels we attach: friend, enemy, or food
  The human capacity for attaching labels has its roots as far back as ant pheromones.
  It hasn’t changed much even in the modern business world:

  * a good colleague is labeled *friend*
  * a dangerous competitor is labeled *enemy*
  * a potential customer or investor is labeled *food*

  Biases are emotions. Every relationship carries its emotional tags (angry, sad, love, fear)
  which tell us which knee-jerk survival reflex to apply to that relationship.
  We need these tags in order to act decisively, which is to say, to survive.
  Without these tags, we suffer the terrible fate of drowning in indecision.
  We can no more shed these biases than we can shed our lungs - biases are essential for survival.

So what is the purpose of "reason why" intelligence?
Biases and emotions are essential for survival, but the can only take us so far.
The ant that smells the "food" label on a poison trap will eat as much as it can.

“Reason why" intelligence is the override circuit on “do or die” circuits. It’s the slow analysis that 
can overcome the edge cases where bias and emotion cause failure.

"Reason why" intelligence is not a replacement for bias, it's an extension. In the course of reasoning,
you reclassify a few labels: with reason, you can deduce that some seeming friends are actually enemies, 
and some seeming food is actually poison. But at the end of the day, "reason why" intelligence is overhead,
and is utterly inadequate for daily survival. Reasoning is very slow and very, very expensive. We don’t have 
the capacity to do much of it, and it’s exhausting. Real-time execution still relies on the biases (the labels) we create.

I think the question we face is not how to eliminate bias, but how to pursue truthful and accurate bias.
Accurate bias means we can act with trust - we greet friends, fight enemies, and eat food, without indecision
or catastrophe. And we live without having to spend our expensive reasoning resources on the norms of daily life.

How do we accurately label “friends”, “enemies” and “food” in a way that we can trust the labels once they’ve been placed?
My new work is tentatively titled “Artificial Trust”, and it’s a challenge to the current neural net AI orthodoxy
which worships “do or die” intelligence at the expense of all reason. Like insects, this neural net form of AI is
incredibly competent... except when it isn’t.

